\documentclass[11pt,]{article}
\usepackage[left=1in,top=1in,right=1in,bottom=1in]{geometry}
\newcommand*{\authorfont}{\fontfamily{phv}\selectfont}
\usepackage[]{mathpazo}


  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}




\usepackage{abstract}
\renewcommand{\abstractname}{}    % clear the title
\renewcommand{\absnamepos}{empty} % originally center

\renewenvironment{abstract}
 {{%
    \setlength{\leftmargin}{0mm}
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \relax}
 {\endlist}

\makeatletter
\def\@maketitle{%
  \newpage
%  \null
%  \vskip 2em%
%  \begin{center}%
  \let \footnote \thanks
    {\fontsize{18}{20}\selectfont\raggedright  \setlength{\parindent}{0pt} \@title \par}%
}
%\fi
\makeatother




\setcounter{secnumdepth}{0}

\usepackage{longtable,booktabs}



\title{Predicting House Prices in Toronto: A Machine Learning Approach  }



\author{\Large Albina Cako, BSc\vspace{0.05in} \newline\normalsize\emph{York University, Certificate in Machine Learning}   \and \Large Colin Green, BSc\vspace{0.05in} \newline\normalsize\emph{York University, Certificate in Machine Learning}   \and \Large Lucy Zhang, BSc\vspace{0.05in} \newline\normalsize\emph{York University, Certificate in Machine Learning}   \and \Large Sean X. Zhang, MSc\vspace{0.05in} \newline\normalsize\emph{York University, Certificate in Machine Learning}  }


\date{}

\usepackage{titlesec}

\titleformat*{\section}{\normalsize\bfseries}
\titleformat*{\subsection}{\normalsize\itshape}
\titleformat*{\subsubsection}{\normalsize\itshape}
\titleformat*{\paragraph}{\normalsize\itshape}
\titleformat*{\subparagraph}{\normalsize\itshape}





\newtheorem{hypothesis}{Hypothesis}
\usepackage{setspace}


% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


% move the hyperref stuff down here, after header-includes, to allow for - \usepackage{hyperref}

\makeatletter
\@ifpackageloaded{hyperref}{}{%
\ifxetex
  \PassOptionsToPackage{hyphens}{url}\usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \PassOptionsToPackage{hyphens}{url}\usepackage[draft,unicode=true]{hyperref}
\fi
}

\@ifpackageloaded{color}{
    \PassOptionsToPackage{usenames,dvipsnames}{color}
}{%
    \usepackage[usenames,dvipsnames]{color}
}
\makeatother
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Albina Cako, BSc (York University, Certificate in Machine Learning) and Colin Green, BSc (York University, Certificate in Machine Learning) and Lucy Zhang, BSc (York University, Certificate in Machine Learning) and Sean X. Zhang, MSc (York University, Certificate in Machine Learning)},
             pdfkeywords = {house prices, machine learning, caret, shiny},  
            pdftitle={Predicting House Prices in Toronto: A Machine Learning Approach},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

% Add an option for endnotes. -----


% add tightlist ----------
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% add some other packages ----------

% \usepackage{multicol}
% This should regulate where figures float
% See: https://tex.stackexchange.com/questions/2275/keeping-tables-figures-close-to-where-they-are-mentioned
\usepackage[section]{placeins}


\begin{document}
	
% \pagenumbering{arabic}% resets `page` counter to 1 
%
% \maketitle

{% \usefont{T1}{pnc}{m}{n}
\setlength{\parindent}{0pt}
\thispagestyle{plain}
{\fontsize{18}{20}\selectfont\raggedright 
\maketitle  % title \par  

}

{
   \vskip 13.5pt\relax \normalsize\fontsize{11}{12} 
\textbf{\authorfont Albina Cako, BSc} \hskip 15pt \emph{\small York University, Certificate in Machine Learning}   \par \textbf{\authorfont Colin Green, BSc} \hskip 15pt \emph{\small York University, Certificate in Machine Learning}   \par \textbf{\authorfont Lucy Zhang, BSc} \hskip 15pt \emph{\small York University, Certificate in Machine Learning}   \par \textbf{\authorfont Sean X. Zhang, MSc} \hskip 15pt \emph{\small York University, Certificate in Machine Learning}   

}

}








\begin{abstract}

    \hbox{\vrule height .2pt width 39.14pc}

    \vskip 8.5pt % \small 

\noindent Lorem Ipsum is simply dummy text of the printing and typesetting
industry. Lorem Ipsum has been the industry's standard dummy text ever
since the 1500s, when an unknown printer took a galley of type and
scrambled it to make a type specimen book. It has survived not only five
centuries, but also the leap into electronic typesetting, remaining
essentially unchanged. It was popularised in the 1960s with the release
of Letraset sheets containing Lorem Ipsum passages, and more recently
with desktop publishing software like Aldus PageMaker including versions
of Lorem Ipsum.


\vskip 8.5pt \noindent \emph{Keywords}: house prices, machine learning, caret, shiny \par

    \hbox{\vrule height .2pt width 39.14pc}



\end{abstract}


\vskip -8.5pt


 % removetitleabstract

\noindent  

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The House Pricing Prediction app is created for estimate the house price
for both buyer and seller based on different factors such as total Sqft,
house locations, etc. The deployment was constructed using ShinyApp. An
user friendly app for both buyer and seller, with simple click of
factors users will get an estimated housing price. The app can be used
for individual buyers who want to know the final price of the houses
they are interested or for individual sellers to know what is the best
listing price. The app uses regression model for prediction, which was
trained by the data set of Toronto housing price. The housing price is
strongly correlated with other factors, for increasing the model
accuracy decreasing errors, it is important to try different factors and
combinations. This project will comprehensively validate four different
models: decision tree, random forest, K nearest neighbors, and gradient
boosting machine. This report will go through data analysis, modeling
implementation and provide an optimistic result for housing price
prediction. \#\# Objective The objective of this project was to evaluate
the application of machine learning algorithms to predict house prices
in the Greater Toronto Area, and apply

\hypertarget{background}{%
\subsection{Background}\label{background}}

Purchasing a house if a big life decision for every individual and needs
a considerable amount of research. Everyone has different purpose of
buying houses, someone would prefer by the house at the best rate for
living now, someone would buy houses for future investment. Selling the
houses is also very important and needs to do research and decide what
is the best leasing price. Commonly, people will ask advise from various
websites, real estate agents or realtors before purchasing or leasing;
However, due to the trend towards big data, house pricing prediction can
be done by using machine learning strategies base on large amount of
data from previous years more correctly. House Price Index (HPI) can
measure the price changes of residential housing as a percentage change,
In Canada the new Housing Price Index is calculated monthly by
Statistics Canada. HPI is useful but because it is a rough indicator
calculated from all transactions, it is inefficient for predicting a
specific house with its attributes. The purpose of this project is to
create an app for both buyers and sellers can easily check the predicted
list price or final price based on the attributes of the house such as
locations, square foot, number of bedrooms, etc.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\hypertarget{data-preprocessing}{%
\subsection{Data Preprocessing}\label{data-preprocessing}}

The housing dataset, originally shared on Github{[}1{]}, was extracted
from Zoocasa.com in the summer of 2019. The dataset contained all
completed property sales in the city of Toronto within a 1-year span. We
performed several data exploration and cleaning steps to prepare this
data for modeling.

\hypertarget{missingness}{%
\subsection{Missingness}\label{missingness}}

We assessed the dataset for missing values. Many parametric machine
learning models do not accept missing data, and the accuracy of even
non-parametric models are often negatively impacted by missingness.
Thus, missing values ought to be either imputed or removed before data
modeling. We then determined whether missing data was Missing Completely
at Random (MCAR), Missing at Random (MAR), or Missing Not at Random
(MNAR). Should the data be MCAR, then it is acceptable to simply remove
each observation that is missing, as doing so would not introduce bias
to the remaining observations. However, if there was a correlation
between missingness and other data features, then imputation must be
performed. Missingness correlation was assessed using the
missing\_compare() function from the finalfit library, which applies the
Kruskal Wallis correlation test for numerical variables and chi-squared
correlation test for categorical variables to determine correlation.
Using the MICE package in R, we then applied the following imputation
methods: 1) simple, which imputes a value from a simple random sample
from the rest of the data; 2) mean, which imputes the average of all
observations; 3) random forest, which applies a random forest algorithm;
and 4) CART, which imputes by classification and regression trees. The
distribution of the imputed data were evaluated with a density plot and
an imputed dataset was chosen based on best fit.

\hypertarget{assessing-parametric-fit}{%
\subsection{Assessing Parametric Fit}\label{assessing-parametric-fit}}

Outliers were visualized with the boxplot() function, with outliers
falling outside Q1 - 1.5 X Inter-Quartile Range and Q3 + 1.5 X
Inter-Quartile Range. Normality of the distribution of variables were
visualized with density plots. A correlogram with Pearson's R determined
collinearity. Linear relationship between outcome variable and
predictors was tested via scatterplots.

\hypertarget{data-curation}{%
\subsection{Data Curation}\label{data-curation}}

The following variables were removed as they did not have any data
utility or had free-text, which would have necessitated additional
cleaning: title, description, mls, type, full\_link, full\_address. A
numeric `bedrooms' column was created by combining bedrooms\_ag and
bedrooms\_bg. We also removed district\_code and city\_district; both
were a categorical variables with number of factors = 140; keeping it
would significantly increase model training time. Longitude and latitude
were also not considered to accommodate model deployment, as including
these variables in training sets would have required geocoding and
assignment of latitude and longitude inputs to districts; tasks which
were outside our scope for this application. Mean\_district\_income was
left as an approximation of the effect of districts on the house price.
After consultation with a real-estate expert, we decreased the number of
property types by generalizing from 10 different types to: Townhouse,
Condo, Detached, Semi-Detached, and Plex

Thus, the predictors chosen were:

\begin{longtable}[]{@{}lll@{}}
\caption{Predictor variables}\tabularnewline
\toprule
Label & & Description\tabularnewline
\midrule
\endfirsthead
\toprule
Label & & Description\tabularnewline
\midrule
\endhead
sqft & & numeric\tabularnewline
beds & & numeric\tabularnewline
bathrooms & & numeric\tabularnewline
parking & & numeric\tabularnewline
mean\_district\_income & & numeric\tabularnewline
type & & categorical\tabularnewline
\bottomrule
\end{longtable}

The target variable chosen was final\_price. Rather than training two
models to predict on both list and final price, the predicted list price
was instead approximated by a linear equation between list and final
price in the training data.

\hypertarget{modeling}{%
\subsection{Modeling}\label{modeling}}

The data contained a mix of categorical and numerical variables and did
not satisfy many requirements of parametric models, such as variable
independence, and normally distributed data. Thus, several parametric
models were used. We trained four different non-parametric models using
k-fold cross validation. The models were then tuned using various grid
searches to improve the accuracy. The final model was chosen based on
three metrics: Root Mean-Squared Error (RMSE), Pearson correlation
(R\^{}2), and Mean Average Error (MAE).

\hypertarget{deployment}{%
\subsection{Deployment}\label{deployment}}

The application was created using R shiny. The user interface (UI)
contains a map of Toronto generated by the leaflet library for
geographic navigation, and allows the user to select various inputs to
predict property price. While the user would choose a district of
interest from the front end, the back end links the district chosen with
income and uses mean\_district\_income as the model input instead. We
chose INSERT\_MODEL HERE, since it was the most accurate model as the
back-end for our application.

\hypertarget{results}{%
\section{Results}\label{results}}

The original housing dataset contained 21 variables and 15234
observations. Table 1 defines each variable of the dataset.

\begin{longtable}[]{@{}lll@{}}
\caption{Data Dictionary}\tabularnewline
\toprule
Label & & Description\tabularnewline
\midrule
\endfirsthead
\toprule
Label & & Description\tabularnewline
\midrule
\endhead
title & & Title of the listing\tabularnewline
final\_price & & Final price of the property\tabularnewline
list\_price & & Listing price of the property\tabularnewline
bedrooms & & Number of bedrooms\tabularnewline
bathrooms & & Number of bathrooms\tabularnewline
sqft & & Area of property in square feet\tabularnewline
parking & & Number of parking spaces\tabularnewline
description & & Verbatim text description of the property\tabularnewline
mls & & MLS ID\tabularnewline
type & & Property type\tabularnewline
full\_link & & URL to listing\tabularnewline
full\_address & & Full address of the property\tabularnewline
lat & & Latitude\tabularnewline
long & & Longitude\tabularnewline
city\_district & & Toronto district to which property belonged
to\tabularnewline
mean\_district\_income & & Average household income of
district\tabularnewline
district\_code & & Numerical code of the district\tabularnewline
final\_price\_transformed & & Box-Cox transformation of final
price\tabularnewline
final\_price\_log & & Log transformation of final price\tabularnewline
bedrooms\_ag & & Number of bedrooms above ground\tabularnewline
bedrooms\_bg & & Number of bedrooms below ground\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{data-exploration}{%
\subsection{Data Exploration}\label{data-exploration}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

The authors would like to thank Hashmat Rohian, adjunct faculty at York
University for supervision of the project. We also thank Slava Spirin
for the original extraction of the Toronto Housing dataset {[}1{]}.
Finally, we thank Steve V. Miller for creation of the manuscript
template in R Markdown {[}2{]}.

\hypertarget{references}{%
\section{References}\label{references}}

\noindent

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-spirin_slavaspirintoronto-housing-price-prediction_2020}{}%
1. Spirin, S. (2020). Slavaspirin/toronto-housing-price-prediction
Available at:
\url{https://github.com/slavaspirin/Toronto-housing-price-prediction}
{[}Accessed October 26, 2020{]}.

\leavevmode\hypertarget{ref-miller_r_nodate}{}%
2. Miller, S.V. An r markdown template for academic manuscripts. Steven
v. Miller. Available at:
\url{http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/}
{[}Accessed October 26, 2020{]}.





\newpage
\singlespacing 
\end{document}

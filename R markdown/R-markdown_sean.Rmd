---
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
title: "Predicting House Prices in Toronto: A Machine Learning Approach"
author:
- name: Albina Cako, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Colin Green, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Lucy Zhang, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Sean X. Zhang, MSc
  affiliation: York University, Certificate in Machine Learning
abstract: "The project focus on building a machine learning model for predicting house price in Toronto. Although  House Price Index (HPI) is commonly used for estimating the changes in housing price but it is not perfect for individual housing price prediction due to high correlation of housing price and other factors such as house location, income distribution, area, and etc. This project evaluates four different model and chooses the best one for deployment with ShinyApp. The app created for both buyer and seller to have an estimated house price due the location and different attributes of the house, and help users to make decisions."
keywords: "house prices, machine learning, caret, shiny"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
bibliography: References_house_prices.bib
csl: cell-numeric.csl
---

# Introduction
The House Pricing Prediction app is created for estimate the house price for both buyer and seller based on different factors such as total Sqft, house locations, etc. The deployment was constructed using ShinyApp. An user friendly app for both buyer and seller, with simple click of factors users will get an estimated housing price. The app can be used for individual buyers who want to know the final price of the houses they are interested or for individual sellers to know what is the best listing price. The app uses regression model for prediction, which was trained by the data set of Toronto housing price. The housing price is strongly correlated with other factors, for increasing the model accuracy decreasing errors, it is important to try different factors and combinations. This project will comprehensively validate four different models: decision tree, random forest, K nearest neighbors, and gradient boosting machine. This report will go through data analysis, modeling implementation and provide an optimistic result for housing price prediction. 

## Objective
The objective of this project was to evaluate the application of machine learning algorithms to predict house prices in the Greater Toronto Area, and apply 

## Background
Purchasing a house is a big life decision for every individual and needs a considerable amount of research. Everyone has different purpose of buying houses, someone would prefer by the house at the best rate for living now, someone would buy houses for future investment. Selling the houses is also very important and needs to do research and decide what is the best leasing price. Commonly, people will ask advise from various websites, real estate agents or realtors before purchasing or leasing; However, due to the trend towards big data, house pricing prediction can be done by using machine learning strategies base on large amount of data from previous years more correctly. House Price Index (HPI) can measure the price changes of residential housing as a percentage change, In Canada the new Housing Price Index is calculated monthly by Statistics Canada. HPI is useful but because it is a rough indicator calculated from all transactions, it is inefficient for predicting a specific house with its attributes. The purpose of this project is to create an app for both buyers and sellers can easily check the predicted list price or final price based on the attributes of the house such as locations, square foot, number of bedrooms, etc.  

# Methodology

## Data Preprocessing
The housing dataset, originally shared on Github[@spirin_slavaspirintoronto-housing-price-prediction_2020], was extracted from Zoocasa.com in the summer of 2019. The dataset contained all completed property sales in the city of Toronto within an approximately 1-year span. We performed several data exploration and cleaning steps to prepare this data for modeling.

## Missingness
We assessed the dataset for missing values. Many parametric machine learning models do not accept missing data, and the accuracy of even non-parametric models are often negatively impacted by missingness. Thus, missing values ought to be either imputed or removed before data modeling. We then determined whether missing data was Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). Should the data be MCAR, then it is acceptable to simply remove each observation that is missing, as doing so would not introduce bias to the remaining observations. However, if there was a correlation between missingness and other data features, then imputation must be performed. Missingness correlation was assessed using the missing_compare() function from the finalfit library, which applies the Kruskal Wallis correlation test for numerical variables and chi-squared correlation test for categorical variables to determine correlation. Using the MICE package in R, we then applied the following imputation methods: 1) simple, which imputes a value from a simple random sample from the rest of the data;
2) mean, which imputes the average of all observations; 3) random forest, which applies a random forest algorithm; and 4) CART, which  imputes by classification and regression trees. The distribution of the imputed data were evaluated with a density plot and an imputed dataset was chosen based on best fit.

## Data Curation

## Modeling
As the data contained a mix of categorical and numerical variables and did not satisfy many requirements of parametric models, such as variable independence, and normally distributed data. Thus, several parametric models were used. We trained four different non-parametric models using k-fold cross validation. The models were then tuned using various grid searches to improve the accuracy. The final model was chosen based on three metrics: Root Mean-Squared Error (RMSE), Pearson correlation (R^2), and Mean Average Error (MAE).

```{r load packages, include=FALSE}
# load the packages
library(dplyr)
library(ggplot2)
library(readr)
library('plot.matrix')
library(caret)
library(gbm)
library(car)
library(pscl)
library(ROCR)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(knitr)
library(VIM)
library(gbm)
library(qwraps2)
library(descr)

```

<<<<<<< HEAD
=======
<<<<<<< HEAD
## Data Preprocessing
The housing dataset, originally shared on Github[@spirin_slavaspirintoronto-housing-price-prediction_2020], was extracted from Zoocasa.com in the summer of 2019. The dataset contained all completed property sales in the city of Toronto within a 1-year span. We performed several data exploration and cleaning steps to prepare this data for modeling.

## Missingness
We assessed the dataset for missing values. Many parametric machine learning models do not accept missing data, and the accuracy of even non-parametric models are often negatively impacted by missingness. Thus, missing values ought to be either imputed or removed before data modeling. We then determined whether missing data was Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). Should the data be MCAR, then it is acceptable to simply remove each observation that is missing, as doing so would not introduce bias to the remaining observations. However, if there was a correlation between missingness and other data features, then imputation must be performed. Missingness correlation was assessed using the missing_compare() function from the finalfit library, which applies the Kruskal Wallis correlation test for numerical variables and chi-squared correlation test for categorical variables to determine correlation. Using the MICE package in R, we then applied the following imputation methods: 1) simple, which imputes a value from a simple random sample from the rest of the data;
2) mean, which imputes the average of all observations; 3) random forest, which applies a random forest algorithm; and 4) CART, which  imputes by classification and regression trees. The distribution of the imputed data were evaluated with a density plot and an imputed dataset was chosen based on best fit.

## Assessing Parametric Fit
Outliers were visualized with the boxplot() function. Data were considered outliers if they were less than Q1 - 1.5 X Inter-Quartile Range and greater Q3 + 1.5 X Inter-Quartile Range. Normality of the distribution of variables were visualized with density plots. A correlogram with Pearson's R determined collinearity. Linear relationship between outcome variable and predictors was tested via scatterplots.

## Data Curation
The following variables were removed as they did not have any data utility or were not easily parseable (i.e. free text): title, description, mls, type, full_link, full_address. A numeric 'bedrooms' column was created by combining bedrooms_ag and bedrooms_bg. We also removed district_code and city_district. Both were categorical variables with number of factors = 140; keeping these would significantly increase model training time. We also did not consider longitude and latitude, as including these variables in training sets would have required geocoding and district clustering; complexities which were outside our scope for this application. Mean_district_income was left as an approximation of the effect of districts on property price. After consultation with a real-estate expert, we decreased the number of property types by generalizing types to: Townhouse, Condo, Detached, Semi-Detached, and Plex. Thus, the predictors chosen were:

```{r predictor table, echo=FALSE, message=FALSE}
predictors_list <- list('sqft', 'beds', 'bathrooms', 'parking','mean_district_income','type')
predictors_type <- list('numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'categorical')
predictors_df <- data.frame(matrix(unlist(predictors_list), nrow=6, byrow=TRUE), stringsAsFactors = FALSE)
predictors_df <- predictors_df %>% mutate(blank = c(" "," "," "," "," "," "),Description = predictors_type)
colnames(predictors_df) <- c("Label", "             ", "Description")
kable(predictors_df, format='pipe', caption = 'Predictor variables')

```

The target variable chosen was final_price. The dataset also contained a list_price variable. Rather than training two models to predict on both list and final price, the predicted list price was instead approximated by a linear equation between list and final price from the original dataset.

## Modeling
The data contained a mix of categorical and numerical variables. These variables did not satisfy the many requirements of parametric models, such as variable independence, normally distributed data, and linear relationship with outcome. Thus, several non-parametric models were used instead. We trained four different models using k-fold cross validation. The models were then tuned using various grid searches to improve the accuracy. The final model was then chosen based on three accuracy metrics: Root Mean-Squared Error (RMSE), Pearson correlation (R^2), and Mean Average Error (MAE).


```{r model table, echo=FALSE, message=FALSE}
models_list <- list('Decision Tree','Random Forest','Gradient Boosting Machines','XGBoost')
models_description <- list('Decision trees repeatedly partition data at specific nodes until the data at the bottom of each branch (known as a leaf) is as homogenous as possible. The model increases in complexity with each additional partition and subsequently becomes more accurate.','Random Forest Model Random Forests are an ensemble learning method for classification and regression. This method will construct a multitude of decision trees and output the mean/average prediction problem for regression or the classes for classification problem. The algorithm can control the number of variable available for splitting at each tree or the number of trees to get a higher accuracy. ',' Gradient Boosting Machines (gbm) begin with creating a preliminary \'weak learner\' decision tree, then sequentially grows more trees that aim to reduce the error of the last one. The algorithm optimizes the loss function by minimizing the residuals at each iteration (difference between predicted and actual value).','sdfsdfgfsdgfdsgsdfgsdfgfsdgsdfgsdfgsdfgsdfgsdfgsdfgsdgsdfdf')
models_params <- list('cp (complexity)','ntree, mtry','n.trees, shrinkage, interaction.depth, n.minobssinnode','4')
models_table <- data.frame(unlist(models_list), unlist(models_description), unlist(models_params))
colnames(models_table) <- c('Model', 'Description', 'Tuning parameters')
kable(models_table, format='pipe', caption = 'Non-parametric Models Used')


```
>>>>>>> 830e5f4958932cc5f824272359d6ab041ceff57f

## Deployment
The application was created using R shiny and hosted on the Shinyapps.io cloud. EDIT : this could probably all go to results section --> The user interface (UI) contains a map of Toronto for geographic navigation and also allows the user to select various inputs to predict property price. While the user would choose a district of interest from the front end, the back end links the district chosen with income and uses mean_district_income as the model input instead. We chose INSERT_MODEL HERE, since it was the most accurate model as the back-end for our application.
=======
>>>>>>> 44c2ed0c3345e5e8d262ee90e1917aa199875ab4

# Results
The original housing dataset contained 21 variables and 15234 observations. Table 1 defines each variable of the dataset.

```{r create a table describing the columns and rows, echo=FALSE}
data_description <- c('Title of the listing', 'Final price of the property', 'Listing price of the property', 'Number of bedrooms','Number of bathrooms','Area of property in square feet',
                      'Number of parking spaces', 'Verbatim text description of the property', 'MLS Listing ID', 'Property type', 'URL to listing', 'Full address of the property', 'Latitude', 'Longitude','Toronto district to which property belonged to', 'Average household income of district', 'Numerical code of the district', 'Box-Cox transformation of final price', 'Log transformation of final price', 'Number of bedrooms above ground', 'Number of bedrooms below ground' )

data <- c("title", "final_price", "list_price", "bedrooms", "bathrooms", "sqft", "parking", "description", "mls", "type", "full_link", "full_address", "lat", "long", "city_district", "mean_district_income", "district_code", "final_price_transformed", "final_price_log", "bedrooms_ag", "bedrooms_bg")

datadictionary <- data.frame(matrix(unlist(data), nrow=21, byrow=TRUE), stringsAsFactors = FALSE)
datadictionary <- datadictionary %>% mutate(blank = c(" "," "," "," "," "," "," "," "," "," "," "," "," "," "," "," "," "," "," "," "," "),Description = data_description)
colnames(datadictionary) <- c("Variable", "             ", "Type")
kable(datadictionary, format='pipe', caption = 'Data Dictionary')

```

## Data Exploration

```{r load data, message=FALSE}
data <- read.csv('houses_edited.csv')
numeric_cols <- list('final_price', 'list_price', 'bathrooms', 'sqft', 'parking', 'lat', 'long', 'mean_district_income', 'bedrooms_ag', 'bedrooms_bg')
predictor_cols <- list('bathrooms', 'sqft', 'parking', 'lat', 'long', 'mean_district_income', 'bedrooms_ag', 'bedrooms_bg')

```


```{r, results = "asis", echo=FALSE}
descr(data)
#summary(data)
```


###

```{r}
#histograms - note skew in prices
par(mfrow=c(2, 5))
for (i in numeric_cols) {
  hist(data[[i]], main = paste(i), xlab = '', col = 4)
}

```

##Modeling and Evaluation
  After data mining and data exploration, the next step is to create the prediction models. There are four models we have decided to use, and we evaluate each model to choose the model has the best performance based on R-Squared , Root Mean Square Error and  Mean Absolute Error.
  
#### **Cross Validation**\
  The k-fold cross-validation method evaluates the model performance on different subset of the training data and then calculate the average prediction error rate. we use k = 10 for our project,and this method was used instead of the simple train-test-split as it gives a more valid estimation of model effectiveness.

#### **Decision Tree Model**\
``````{r Decision Tree Model, echo=FALSE}
decision_tree_model <- readRDS("decision_tree_model.rds")

decision_tree_model$final_model

```

#### **Random Forest Model**\
``````{r Random Forest Model, echo=FALSE}
decision_tree_model <- readRDS("randomForests.rds")

print(randomForests)
```

#### **Gradient Boosting Model**\
```````{r Gradient Boosting Model, echo=FALSE}
decision_tree_model <- readRDS("gbm3.rds")

gbm3$final_model

```

#### **Extreme Gradient Boosting Model**\
```````{r Extreme Gradient Boosting Model, echo=FALSE}
decision_tree_model <- readRDS("albina_model.rds")

albina_model$final_model
```

### **Model Selection**\
  For the four models, we chooses the extreme gradient boosting model as our final model for deployment. As you can see, the extreme gradient boosting model has the highest R-Squared and the lowest Mean Absolute value.





# Discussion

# Acknowledgements

The authors would like to thank Hashmat Rohian, adjunct faculty at York University for supervision of the project. We also thank Slava Spirin for the original extraction of the Toronto Housing dataset [@spirin_slavaspirintoronto-housing-price-prediction_2020]. Finally, we thank Steve V. Miller for creation of the manuscript template in R Markdown [@miller_r_nodate].

# References
\noindent
